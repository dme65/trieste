:py:mod:`trieste.acquisition.sampler`
=====================================

.. py:module:: trieste.acquisition.sampler

.. autoapi-nested-parse::

   This module is the home of the sampling functionality required by Trieste's
   acquisiiton functions.



Module Contents
---------------

.. py:class:: Sampler(sample_size: int, model: trieste.models.ProbabilisticModel)

   Bases: :py:obj:`abc.ABC`

   A :class:`Sampler` samples a specific quantity across a discrete set of points
   according to an underlying :class:`ProbabilisticModel`.

   :param sample_size: The desired number of samples.
   :param model: The model to sample from.
   :raise ValueError (or InvalidArgumentError): If ``sample_size`` is not positive.

   .. py:method:: sample(self, at: trieste.types.TensorType) -> trieste.types.TensorType
      :abstractmethod:

      :param at: Input points that define the sampler.
      :return: Samples.



.. py:class:: ThompsonSampler(sample_size: int, model: trieste.models.ProbabilisticModel, sample_min_value: bool = False)

   Bases: :py:obj:`Sampler`, :py:obj:`abc.ABC`

   A :class:`ThompsonSampler` samples either the minimum values or minimisers of a function
   modeled by an underlying :class:`ProbabilisticModel` across a  discrete set of points.


   :param sample_size: The desired number of samples.
   :param model: The model to sample from.
   :sample_min_value: If True then sample from the minimum value of the function,
       else sample the function's minimiser.
   :raise ValueError (or InvalidArgumentError): If ``sample_size`` is not positive.


.. py:class:: ExactThompsonSampler(sample_size: int, model: trieste.models.ProbabilisticModel, sample_min_value: bool = False)

   Bases: :py:obj:`ThompsonSampler`

   This sampler provides exact Thompson samples of the objective function's
   maximiser :math:`x^*` over a discrete set of input locations.

   Although exact Thompson sampling is costly (incuring with an :math:`O(N^3)` complexity to
   sample over a set of `N` locations), this method can be used for any probabilistic model
   with a sampling method.


   :param sample_size: The desired number of samples.
   :param model: The model to sample from.
   :sample_min_value: If True then sample from the minimum value of the function,
       else sample the function's minimiser.
   :raise ValueError (or InvalidArgumentError): If ``sample_size`` is not positive.

   .. py:method:: sample(self, at: trieste.types.TensorType) -> trieste.types.TensorType

      Return exact samples from either the objective function's minimser or its minimal value
      over the candidate set `at`.

      :param at: Where to sample the predictive distribution, with shape `[N, D]`, for points
          of dimension `D`.
      :return: The samples, of shape `[S, D]` (where `S` is the `sample_size`) if sampling
          the function's minimser or shape `[S, 1]` if sampling the function's mimimal value.
      :raise ValueError (or InvalidArgumentError): If ``at`` has an invalid shape.



.. py:class:: GumbelSampler(sample_size: int, model: trieste.models.ProbabilisticModel)

   Bases: :py:obj:`ThompsonSampler`

   This sampler follows :cite:`wang2017max` and yields approximate samples of the objective
   minimum value :math:`y^*` via the empirical cdf :math:`\operatorname{Pr}(y^*<y)`. The cdf
   is approximated by a Gumbel distribution

   .. math:: \mathcal G(y; a, b) = 1 - e^{-e^\frac{y - a}{b}}

   where :math:`a, b \in \mathbb R` are chosen such that the quartiles of the Gumbel and cdf match.
   Samples are obtained via the Gumbel distribution by sampling :math:`r` uniformly from
   :math:`[0, 1]` and applying the inverse probability integral transform
   :math:`y = \mathcal G^{-1}(r; a, b)`.

   Note that the :class:`GumbelSampler` can only sample a function's minimal value and not
   its minimiser.

   :param sample_size: The desired number of samples.
   :param model: The model to sample from.
   :raise ValueError (or InvalidArgumentError): If ``sample_size`` is not positive.

   .. py:method:: sample(self, at: trieste.types.TensorType) -> trieste.types.TensorType

      Return approximate samples from of the objective function's minimum value.

      :param at: Points at where to fit the Gumbel distribution, with shape `[N, D]`, for points
          of dimension `D`. We recommend scaling `N` with search space dimension.
      :return: The samples, of shape `[S, 1]`, where `S` is the `sample_size`.
      :raise ValueError (or InvalidArgumentError): If ``at`` has an invalid shape.



.. py:class:: IndependentReparametrizationSampler(sample_size: int, model: trieste.models.ProbabilisticModel)

   Bases: :py:obj:`Sampler`

   This sampler employs the *reparameterization trick* to approximate samples from a
   :class:`ProbabilisticModel`\ 's predictive distribution as

   .. math:: x \mapsto \mu(x) + \epsilon \sigma(x)

   where :math:`\epsilon \sim \mathcal N (0, 1)` is constant for a given sampler, thus ensuring
   samples form a continuous curve.

   :param sample_size: The number of samples to take at each point. Must be positive.
   :param model: The model to sample from.
   :raise ValueError (or InvalidArgumentError): If ``sample_size`` is not positive.

   .. py:method:: sample(self, at: trieste.types.TensorType) -> trieste.types.TensorType

      Return approximate samples from the `model` specified at :meth:`__init__`. Multiple calls to
      :meth:`sample`, for any given :class:`IndependentReparametrizationSampler` and ``at``, will
      produce the exact same samples. Calls to :meth:`sample` on *different*
      :class:`IndependentReparametrizationSampler` instances will produce different samples.

      :param at: Where to sample the predictive distribution, with shape `[..., 1, D]`, for points
          of dimension `D`.
      :return: The samples, of shape `[..., S, 1, L]`, where `S` is the `sample_size` and `L` is
          the number of latent model dimensions.
      :raise ValueError (or InvalidArgumentError): If ``at`` has an invalid shape.



.. py:class:: BatchReparametrizationSampler(sample_size: int, model: trieste.models.ProbabilisticModel)

   Bases: :py:obj:`Sampler`

   This sampler employs the *reparameterization trick* to approximate batches of samples from a
   :class:`ProbabilisticModel`\ 's predictive joint distribution as

   .. math:: x \mapsto \mu(x) + \epsilon L(x)

   where :math:`L` is the Cholesky factor s.t. :math:`LL^T` is the covariance, and
   :math:`\epsilon \sim \mathcal N (0, 1)` is constant for a given sampler, thus ensuring samples
   form a continuous curve.

   :param sample_size: The number of samples for each batch of points. Must be positive.
   :param model: The model to sample from.
   :raise ValueError (or InvalidArgumentError): If ``sample_size`` is not positive.

   .. py:method:: sample(self, at: trieste.types.TensorType, *, jitter: float = DEFAULTS.JITTER) -> trieste.types.TensorType

      Return approximate samples from the `model` specified at :meth:`__init__`. Multiple calls to
      :meth:`sample`, for any given :class:`BatchReparametrizationSampler` and ``at``, will
      produce the exact same samples. Calls to :meth:`sample` on *different*
      :class:`BatchReparametrizationSampler` instances will produce different samples.

      :param at: Batches of query points at which to sample the predictive distribution, with
          shape `[..., B, D]`, for batches of size `B` of points of dimension `D`. Must have a
          consistent batch size across all calls to :meth:`sample` for any given
          :class:`BatchReparametrizationSampler`.
      :param jitter: The size of the jitter to use when stabilising the Cholesky decomposition of
          the covariance matrix.
      :return: The samples, of shape `[..., S, B, L]`, where `S` is the `sample_size`, `B` the
          number of points per batch, and `L` the dimension of the model's predictive
          distribution.
      :raise ValueError (or InvalidArgumentError): If any of the following are true:
          - ``at`` is a scalar.
          - The batch size `B` of ``at`` is not positive.
          - The batch size `B` of ``at`` differs from that of previous calls.
          - ``jitter`` is negative.



.. py:data:: TrajectoryFunction
   

   Type alias for trajectory functions.

   An :const:`TrajectoryFunction` evaluates a particular sample at a set of `N` query
   points (each of dimension `D`) i.e. takes input of shape `[N, D]` and returns
   shape `[N, 1]`.

   A key property of these trajectory functions is that the same sample draw is evaluated
   for all queries. This property is known as consistency.


.. py:class:: RandomFourierFeatureThompsonSampler(sample_size: int, model: trieste.models.ProbabilisticModel, dataset: trieste.data.Dataset, sample_min_value: bool = False, num_features: int = 1000)

   Bases: :py:obj:`ThompsonSampler`

   This class builds functions that approximate a trajectory sampled from an underlying Gaussian
   process model. For tractibility, the Gaussian process is approximated with a Bayesian
   Linear model across a set of features sampled from the Fourier feature decomposition of
   the model's kernel. See :cite:`hernandez2014predictive` for details.

   Achieving consistency (ensuring that the same sample draw for all evalutions of a particular
   trajectory function) for exact sample draws from a GP is prohibitively costly because it scales
   cubically with the number of query points. However, finite feature representations can be
   evaluated with constant cost regardless of the required number of queries.

   In particular, we approximate the Gaussian processes' posterior samples as the finite feature
   approximation

   .. math:: \hat{f}(x) = \sum_{i=1}^m \phi_i(x)\theta_i

   where :math:`\phi_i` are m Fourier features and :math:`\theta_i` are
   feature weights sampled from a posterior distribution that depends on the feature values at the
   model's datapoints.

   Our implementation follows :cite:`hernandez2014predictive`, with our calculations
   differing slightly depending on properties of the problem. In particular,  we used different
   calculation strategies depending on the number of considered features m and the number
   of data points n.

   If :math:`m<n` then we follow Appendix A of :cite:`hernandez2014predictive` and calculate the
   posterior distribution for :math:`\theta` following their Bayesian linear regression motivation,
   i.e. the computation revolves around an O(m^3)  inversion of a design matrix.

   If :math:`n<m` then we use the kernel trick to recast computation to revolve around an O(n^3)
   inversion of a gram matrix. As well as being more efficient in early BO
   steps (where :math:`n<m`), this second computation method allows must larger choices
   of m (as required to approximate very flexible kernels).

   :param sample_size: The desired number of samples.
   :param model: The model to sample from.
   :param dataset: The data from the observer. Must be populated.
   :sample_min_value: If True then sample from the minimum value of the function,
       else sample the function's minimiser.
   :param num_features: The number of features used to approximate the kernel. We use a default
       of 1000 as it typically perfoms well for a wide range of kernels. Note that very smooth
       kernels (e.g. RBF) can be well-approximated with fewer features.
   :raise ValueError: If ``dataset`` is empty.

   .. py:method:: get_trajectory(self) -> TrajectoryFunction

      Generate an approximate function draw (trajectory) by sampling weights
      and evaluating the feature functions.

      :return: A trajectory function representing an approximate trajectory from the Gaussian
          process, taking an input of shape `[N, D]` and returning shape `[N, 1]`


   .. py:method:: sample(self, at: trieste.types.TensorType) -> trieste.types.TensorType

      Return approximate samples from either the objective function's minimser or its minimal
      value over the candidate set `at`.

      :param at: Where to sample the predictive distribution, with shape `[N, D]`, for points
          of dimension `D`.
      :return: The samples, of shape `[S, D]` (where `S` is the `sample_size`) if sampling
          the function's minimser or shape `[S, 1]` if sampling the function's mimimal value.
      :raise ValueError (or InvalidArgumentError): If ``at`` has an invalid shape.



